{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256246e3-c5dd-4480-86b5-2f14b3566741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen_scaled: double (nullable = true)\n",
      " |-- tenure_scaled: double (nullable = true)\n",
      " |-- MonthlyCharges_scaled: double (nullable = true)\n",
      " |-- TotalCharges_scaled: double (nullable = true)\n",
      " |-- Partner_indexed: double (nullable = true)\n",
      " |-- Dependents_indexed: double (nullable = true)\n",
      " |-- PhoneService_indexed: double (nullable = true)\n",
      " |-- MultipleLines_indexed: double (nullable = true)\n",
      " |-- InternetService_indexed: double (nullable = true)\n",
      " |-- OnlineSecurity_indexed: double (nullable = true)\n",
      " |-- OnlineBackup_indexed: double (nullable = true)\n",
      " |-- DeviceProtection_indexed: double (nullable = true)\n",
      " |-- TechSupport_indexed: double (nullable = true)\n",
      " |-- StreamingTV_indexed: double (nullable = true)\n",
      " |-- StreamingMovies_indexed: double (nullable = true)\n",
      " |-- Contract_indexed: double (nullable = true)\n",
      " |-- PaperlessBilling_indexed: double (nullable = true)\n",
      " |-- PaymentMethod_indexed: double (nullable = true)\n",
      " |-- Churn_indexed: double (nullable = true)\n",
      "\n",
      "Accuracy: 0.8111782477341389\n",
      "Recall for Class 0: 0.902020202020202\n",
      "Recall for Class 1: 0.5419161676646707\n",
      "F1 Score Macro: 0.8051361068369995\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('LogisticRegressionWithMLflow').getOrCreate()\n",
    "input_data = spark.read.csv(\"../data/processed/data.csv\", header=True, inferSchema=True)\n",
    "# input_data.printSchema()\n",
    "\n",
    "feature_cols = [col for col in input_data.columns if col != 'Churn_indexed']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "train_data, test_data = input_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Logistic Regression\n",
    "params = {\n",
    "    \"featuresCol\": \"features\",\n",
    "    \"labelCol\": \"Churn_indexed\",\n",
    "    \"maxIter\": 1000\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(**params)\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_indexed', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "recall_class_0 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 0})\n",
    "recall_class_1 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})\n",
    "f1_score_macro = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall for Class 0: {recall_class_0}\")\n",
    "print(f\"Recall for Class 1: {recall_class_1}\")\n",
    "print(f\"F1 Score Macro: {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2de5495-f673-47ce-9c51-1847d5fb172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7990936555891238\n",
      "Recall for Class 0: 0.8878787878787879\n",
      "Recall for Class 1: 0.5359281437125748\n",
      "F1 Score Macro: 0.7941941837006201\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_params = {\n",
    "    \"featuresCol\": \"features\",\n",
    "    \"labelCol\": \"Churn_indexed\",\n",
    "    \"maxDepth\": 5, \n",
    "    \"impurity\": \"gini\"\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(**dt_params)\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_indexed', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "recall_class_0 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 0})\n",
    "recall_class_1 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})\n",
    "f1_score_macro = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall for Class 0: {recall_class_0}\")\n",
    "print(f\"Recall for Class 1: {recall_class_1}\")\n",
    "print(f\"F1 Score Macro: {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c084f940-c72a-41a8-85c9-f1d582e3bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8006042296072508\n",
      "Recall for Class 0: 0.9414141414141414\n",
      "Recall for Class 1: 0.38323353293413176\n",
      "F1 Score Macro: 0.7791625531409281\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_params = {\n",
    "    \"featuresCol\": \"features\",\n",
    "    \"labelCol\": \"Churn_indexed\",\n",
    "    \"numTrees\": 100,  # Number of trees in the forest\n",
    "    \"maxDepth\": 5,    # Maximum depth of each tree\n",
    "    \"impurity\": \"gini\",\n",
    "    \"featureSubsetStrategy\": \"auto\"  # Auto strategy for feature subset for training each tree\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_indexed', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "recall_class_0 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 0})\n",
    "recall_class_1 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})\n",
    "f1_score_macro = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall for Class 0: {recall_class_0}\")\n",
    "print(f\"Recall for Class 1: {recall_class_1}\")\n",
    "print(f\"F1 Score Macro: {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f1d6f5b-9373-4c3b-9fe6-23a3428955f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8089123867069486\n",
      "Recall for Class 0: 0.9222222222222223\n",
      "Recall for Class 1: 0.47305389221556887\n",
      "F1 Score Macro: 0.7968384769206691\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Linear SVC Parameters\n",
    "svc_params = {\n",
    "    \"featuresCol\": \"features\",\n",
    "    \"labelCol\": \"Churn_indexed\",\n",
    "    \"maxIter\": 100,\n",
    "    \"regParam\": 0.1  # Regularization parameter\n",
    "}\n",
    "\n",
    "svc = LinearSVC(**svc_params)\n",
    "pipeline = Pipeline(stages=[assembler, svc])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_indexed', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "recall_class_0 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 0})\n",
    "recall_class_1 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})\n",
    "f1_score_macro = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall for Class 0: {recall_class_0}\")\n",
    "print(f\"Recall for Class 1: {recall_class_1}\")\n",
    "print(f\"F1 Score Macro: {f1_score_macro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12250683-077f-4d80-90ba-579256c4d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8285498489425982\n",
      "Recall for Class 0: 0.901010101010101\n",
      "Recall for Class 1: 0.6137724550898204\n",
      "F1 Score Macro: 0.8256994555668637\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# GBT Classifier\n",
    "gbt_params = {\n",
    "    \"featuresCol\": \"features\",\n",
    "    \"labelCol\": \"Churn_indexed\",\n",
    "    \"maxIter\": 100,\n",
    "    \"maxDepth\": 5,\n",
    "    \"lossType\": \"logistic\"\n",
    "}\n",
    "\n",
    "gbt = GBTClassifier(**gbt_params)\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_indexed', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "recall_class_0 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 0})\n",
    "recall_class_1 = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1})\n",
    "f1_score_macro = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall for Class 0: {recall_class_0}\")\n",
    "print(f\"Recall for Class 1: {recall_class_1}\")\n",
    "print(f\"F1 Score Macro: {f1_score_macro}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d46838-e185-431e-8110-8c6ba282a7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
